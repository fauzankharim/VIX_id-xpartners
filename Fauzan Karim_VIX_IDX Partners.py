# -*- coding: utf-8 -*-
"""Untitled15.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lNHBoLjcPm5JW0c4YN-89FBNbkJqjFbW

# Introduction

**Problem:** We are investors in P2P platforms. There are loans that get charged off in the end. If a loan get charged off or defaulted, we will lost our money. We want to prevent that, and minimize our loss.

**Business Metrics:** Loss, net profit margin.

**Solution explanation:** We will create a machine learning model that can identify if a loan is potentially bad / risky loan. It can be used as an investment decision tools. For the model, we're gonna use some non-parametrical algorithm (with little assumption) because we are not statistician, and statistics is hard. If our model is reliable, our investment in risky loans will decrease, our loss can be minimized and our net profit margin should increase.

**Data:** Lending club credit loan data between 2007 - 2014

# Import and Load Dataset
"""

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import seaborn as sns # cool graph
import matplotlib.pyplot as plt # graph

from datetime import datetime as dt
from collections import defaultdict
import time
from warnings import filterwarnings
filterwarnings('ignore')

df = pd.read_csv('loan_data_2007_2014.csv', sep=',')
df.sample(5)

df.info()

df.shape

"""# Statistical Summary"""

numerical = df.select_dtypes(exclude='object')
categorical = df.select_dtypes(include='object')

numerical.describe()

categorical.describe()

cols_to_drop = [
    'Unnamed: 0',
    # unique id
    'id'
    , 'member_id'
    
    # free text
    , 'url'
    , 'desc'
    
    # all null / constant / others
    , 'zip_code' 
    , 'annual_inc_joint'
    , 'dti_joint'
    , 'verification_status_joint'
    , 'open_acc_6m'
    , 'open_il_6m'
    , 'open_il_12m'
    , 'open_il_24m'
    , 'mths_since_rcnt_il'
    , 'total_bal_il'
    , 'il_util'
    , 'open_rv_12m'
    , 'open_rv_24m'
    , 'max_bal_bc'
    , 'all_util'
    , 'inq_fi'
    , 'total_cu_tl'
    , 'inq_last_12m'
    
    # expert judgment
    , 'sub_grade'
]

df = df.drop(cols_to_drop, axis=1)

df.info()

"""# Target and Labeling"""

df.loan_status.value_counts(normalize=True)*100

bad_status = [
    'Charged Off' 
    , 'Default' 
    , 'Does not meet the credit policy. Status:Charged Off'
    , 'Late (31-120 days)'
]

df['bad_credit'] = np.where(df['loan_status'].isin(bad_status), 1, 0)

df['bad_credit'].value_counts(normalize=True)*100

df.drop('loan_status', axis=1, inplace=True)

"""# Preprocessing

## Feature Engineering

### emp_length
"""

df['emp_length'].unique()

df['emp_length'] = df['emp_length'].str.replace('\+ years', '')
df['emp_length'] = df['emp_length'].str.replace('< 1 year', str(0))
df['emp_length'] = df['emp_length'].str.replace(' years', '')
df['emp_length'] = df['emp_length'].str.replace(' year', '')

df['emp_length'] = df['emp_length'].astype(float)

df['emp_length'].describe()

"""### term"""

df['term'].unique()

df['term'] = df['term'].str.replace(' months', '')
df['term'] = df['term'].astype(float)

df['term'].describe()

"""### earliest_cr_line"""

df['earliest_cr_line'].head(5)

df['earliest_cr_line_date'] = pd.to_datetime(df['earliest_cr_line'], format='%b-%y')
df['earliest_cr_line_date'].head(3)

df['mths_earliest_cr_line'] = round(pd.to_numeric((pd.to_datetime('2017-12-31') - df['earliest_cr_line_date']) / np.timedelta64(1, 'M')))
df['mths_earliest_cr_line'].head(3)

df['mths_earliest_cr_line'].describe()

df[df['mths_earliest_cr_line']<0][['earliest_cr_line', 'earliest_cr_line_date', 'mths_earliest_cr_line']].head(3)

df.loc[df['mths_earliest_cr_line']<0, 'mths_earliest_cr_line'] = df['mths_earliest_cr_line'].max()

df.drop(['earliest_cr_line', 'earliest_cr_line_date'], axis=1, inplace=True)

df['mths_earliest_cr_line'].describe()

"""### issue_d"""

df['issue_d_date'] = pd.to_datetime(df['issue_d'], format='%b-%y')
df['mths_issue_d'] = round(pd.to_numeric((pd.to_datetime('2017-12-31') - df['issue_d_date']) / np.timedelta64(1, 'M')))

df.drop(['issue_d', 'issue_d_date'], axis=1, inplace=True)

df['mths_issue_d'].describe()

"""### last_pymnt_d"""

df['last_pymnt_d_date'] = pd.to_datetime(df['last_pymnt_d'], format='%b-%y')
df['mths_last_pymnt_d'] = round(pd.to_numeric((pd.to_datetime('2017-12-31') - df['last_pymnt_d_date']) / np.timedelta64(1, 'M')))

df.drop(['last_pymnt_d', 'last_pymnt_d_date'], axis=1, inplace=True)

df['mths_last_pymnt_d'].describe()

"""### next_pymnt_d"""

df['next_pymnt_d_date'] = pd.to_datetime(df['next_pymnt_d'], format='%b-%y')
df['mths_next_pymnt_d'] = round(pd.to_numeric((pd.to_datetime('2017-12-31') - df['next_pymnt_d_date']) / np.timedelta64(1, 'M')))

df.drop(['next_pymnt_d', 'next_pymnt_d_date'], axis=1, inplace=True)

df['mths_next_pymnt_d'].describe()

"""### last_credit_pull_d"""

df['last_credit_pull_d_date'] = pd.to_datetime(df['last_credit_pull_d'], format='%b-%y')
df['mths_last_credit_pull_d'] = round(pd.to_numeric((pd.to_datetime('2017-12-31') - df['last_credit_pull_d_date']) / np.timedelta64(1, 'M')))

df.drop(['last_credit_pull_d', 'last_credit_pull_d_date'], axis=1, inplace=True)

df['mths_last_credit_pull_d'].describe()

"""## Handling Missing Values

### Check
"""

# print the name of columns with missing values (so I can copy paste :D)
missing = df.isnull().sum() * 100 / df.shape[0]
missing[missing > 0].sort_values(ascending=False)

df.drop(['mths_since_last_record', 'mths_since_last_major_derog'], axis=1, inplace=True)

df.isnull().sum()

"""Fill Null-Values 1"""

df['term'].fillna(df['term'].mean(), inplace=True)
df['int_rate'].fillna(df['int_rate'].mean(), inplace=True)
df['installment'].fillna(df['installment'].mean(), inplace=True)
df['grade'].fillna('B', inplace=True)
df['emp_title'].fillna('US Army', inplace=True)
df['emp_length'].fillna(df['emp_length'].mean(), inplace=True)
df['home_ownership'].fillna('MORTAGE', inplace=True)
df['annual_inc'].fillna(df['annual_inc'].mean(), inplace=True)
df['verification_status'].fillna('Verified', inplace=True)
df['pymnt_plan'].fillna('n', inplace=True)
df['purpose'].fillna('debt_consolidation', inplace=True)
df['title'].fillna('Debt Consolidation Loan', inplace=True)

"""Fill Null-Values 2"""

df['addr_state'].fillna('CA', inplace=True)
df['dti'].fillna(df['dti'].mean(), inplace=True)
df['delinq_2yrs'].fillna(df['delinq_2yrs'].mean(), inplace=True)
df['inq_last_6mths'].fillna(df['inq_last_6mths'].mean(), inplace=True)
df['mths_since_last_delinq'].fillna(df['mths_since_last_delinq'].mean(), inplace=True)
df['open_acc'].fillna(df['open_acc'].mean(), inplace=True)
df['pub_rec'].fillna(df['pub_rec'].mean(), inplace=True)
df['revol_bal'].fillna(df['revol_bal'].mean(), inplace=True)
df['revol_util'].fillna(df['revol_util'].mean(), inplace=True)
df['total_acc'].fillna(df['total_acc'].mean(), inplace=True)
df['initial_list_status'].fillna('f', inplace=True)
df['out_prncp'].fillna(df['out_prncp'].mean(), inplace=True)

"""Fill Null-Values 3"""

df['out_prncp_inv'].fillna(df['out_prncp_inv'].mean(), inplace=True)
df['total_pymnt'].fillna(df['total_pymnt'].mean(), inplace=True)
df['total_pymnt_inv'].fillna(df['total_pymnt_inv'].mean(), inplace=True)
df['total_rec_prncp'].fillna(df['total_rec_prncp'].mean(), inplace=True)
df['total_rec_int'].fillna(df['total_rec_int'].mean(), inplace=True)
df['total_rec_late_fee'].fillna(df['total_rec_late_fee'].mean(), inplace=True)
df['recoveries'].fillna(df['recoveries'].mean(), inplace=True)
df['collection_recovery_fee'].fillna(df['collection_recovery_fee'].mean(), inplace=True)
df['last_pymnt_amnt'].fillna(df['last_pymnt_amnt'].mean(), inplace=True)
df['collections_12_mths_ex_med'].fillna(df['collections_12_mths_ex_med'].mean(), inplace=True)
df['policy_code'].fillna(df['policy_code'].mean(), inplace=True)
df['application_type'].fillna('INDIVIDUAL', inplace=True)

"""Fill Null-Values 4"""

df['acc_now_delinq'].fillna(df['acc_now_delinq'].mean(), inplace=True)
df['tot_coll_amt'].fillna(df['tot_coll_amt'].mean(), inplace=True)
df['tot_cur_bal'].fillna(df['tot_cur_bal'].mean(), inplace=True)
df['bad_credit'].fillna(df['bad_credit'].mean(), inplace=True)
df['total_rev_hi_lim'].fillna(df['total_rev_hi_lim'].mean(), inplace=True)
df['mths_earliest_cr_line'].fillna(df['mths_earliest_cr_line'].mean(), inplace=True)
df['mths_issue_d'].fillna(df['mths_issue_d'].mean(), inplace=True)
df['mths_last_pymnt_d'].fillna(df['mths_last_pymnt_d'].mean(), inplace=True)
df['mths_next_pymnt_d'].fillna(df['mths_next_pymnt_d'].mean(), inplace=True)
df['mths_last_credit_pull_d'].fillna(df['mths_last_credit_pull_d'].mean(), inplace=True)

df.drop(['tot_coll_amt'], axis=1, inplace=True)
df.drop(['tot_cur_bal'], axis=1, inplace=True)
df.drop(['total_rev_hi_lim'], axis=1, inplace=True)

"""# EDA

## Correlation
"""

plt.figure(figsize=(20,20))
sns.heatmap(df.corr())

corr_matrix = df.corr().abs()
upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))
to_drop_hicorr = [column for column in upper.columns if any(upper[column] > 0.7)]

to_drop_hicorr

df.drop(to_drop_hicorr, axis=1, inplace=True)

"""## Feature Selection"""

categorical = df.select_dtypes(include='object').nunique()
categorical

numerical = df.select_dtypes(exclude='object').nunique()
numerical

for col in df.select_dtypes(include='object').columns.tolist():
    print(df[col].value_counts(normalize=True)*100)
    print('\n')

"""# Scalling and Transformation

## One Hot Encoding
"""

categorical = [col for col in df.select_dtypes(include='object').columns.tolist()]

onehot = pd.get_dummies(df[categorical], drop_first=True)

onehot.head()

"""## Standarization"""

numerical = [col for col in df.columns.tolist() if col not in categorical + ['bad_credit']]

from sklearn.preprocessing import StandardScaler

ss = StandardScaler()
std = pd.DataFrame(ss.fit_transform(df[numerical]), columns=numerical)

std.head()

"""##Data Transformation"""

df_model = pd.concat([onehot, std, df[['bad_credit']]], axis=1)

"""#Train Test Split"""

from sklearn.model_selection import train_test_split

X = df_model.drop('bad_credit', axis=1)
y = df_model['bad_credit']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train.shape, X_test.shape

"""# Modeling"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression

from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score
from sklearn.metrics import roc_curve, auc
from scipy.stats import kstest
from sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay

"""## Evaluation"""

def evaluation(X_train,X_test,y_train,y_test):
    """
    This function want to do an experiment for several models.
    We just need data input

    Parameter
    ---------
    X_train = training data contains several features
    X_test = testing data contains several features
    y_train = train target
    y_test = test target
    """
    result = defaultdict(list)
    
    logreg = LogisticRegression()
    dtc = DecisionTreeClassifier()
    rf = RandomForestClassifier()
  

    list_model = [('Logistic Regression',logreg),
                  ('Decision Tree',dtc),
                  ('Random Forest',rf),
                 ]
    
    for model_name,model in list_model:
        start = dt.now()
        model.fit(X_train,y_train)
        duration = (dt.now()-start).total_seconds()
        
        y_pred = model.predict(X_test)
        y_pred_proba = model.predict_proba(X_test)[:][:,1]
        
        df_actual_predicted = pd.concat([pd.DataFrame(np.array(y_test), columns=['y_actual']), pd.DataFrame(y_pred_proba, columns=['y_pred_proba'])], axis=1)
        df_actual_predicted.index = y_test.index
        
        df_actual_predicted['Cumulative N Population'] = df_actual_predicted.index + 1
        df_actual_predicted['Cumulative N Bad'] = df_actual_predicted['y_actual'].cumsum()
        df_actual_predicted['Cumulative N Good'] = df_actual_predicted['Cumulative N Population'] - df_actual_predicted['Cumulative N Bad']
        df_actual_predicted['Cumulative Perc Population'] = df_actual_predicted['Cumulative N Population'] / df_actual_predicted.shape[0]
        df_actual_predicted['Cumulative Perc Bad'] = df_actual_predicted['Cumulative N Bad'] / df_actual_predicted['y_actual'].sum()
        df_actual_predicted['Cumulative Perc Good'] = df_actual_predicted['Cumulative N Good'] / (df_actual_predicted.shape[0] - df_actual_predicted['y_actual'].sum())
        
        accuracy = accuracy_score(y_test,y_pred)
        recall = recall_score(y_test,y_pred)
        precision = precision_score(y_test,y_pred)
        f_score = f1_score(y_test, y_pred)
        fpr, tpr, tr = roc_curve(df_actual_predicted['y_actual'], df_actual_predicted['y_pred_proba'])
        auc = roc_auc_score(df_actual_predicted['y_actual'], df_actual_predicted['y_pred_proba'])
        kolsmir = max(df_actual_predicted['Cumulative Perc Good'] - df_actual_predicted['Cumulative Perc Bad'])/10
        
        
        result['model_name'].append(model_name)
        result['model'].append(model)
        result['accuracy'].append(accuracy)
        result['recall'].append(recall)
        result['precision'].append(precision)
        result['f1_score'].append(f_score)
        result['AUC'].append(auc)
        result['KS'].append(kolsmir)
        result['duration'].append(duration)
        
    return result

result = evaluation(X_train,X_test,y_train,y_test)
result = pd.DataFrame(result)

result

"""## Feature Importance"""

rfc = RandomForestClassifier(max_depth=4)
rfc.fit(X_train, y_train)

arr_feature_importances = rfc.feature_importances_
arr_feature_names = X_train.columns.values
    
df_feature_importance = pd.DataFrame(index=range(len(arr_feature_importances)), columns=['feature', 'importance'])
df_feature_importance['feature'] = arr_feature_names
df_feature_importance['importance'] = arr_feature_importances
df_all_features = df_feature_importance.sort_values(by='importance', ascending=False)
df_all_features

